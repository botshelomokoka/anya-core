name: Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * 0'  # Run weekly on Sunday
  workflow_dispatch:

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for accurate benchmarking

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-benchmark memory_profiler psutil

      - name: Cache benchmark data
        uses: actions/cache@v3
        with:
          path: .benchmarks
          key: ${{ runner.os }}-benchmark-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-benchmark-

      - name: Run AI Model Performance Tests
        run: |
          pytest benchmarks/test_ai_performance.py --benchmark-only --benchmark-json output.json
        continue-on-error: true

      - name: Run Memory Usage Tests
        run: |
          python -m memory_profiler benchmarks/memory_profile.py > memory_report.txt

      - name: Run Load Tests
        run: |
          python benchmarks/load_test.py --duration 300 --users 100 > load_test_report.txt

      - name: Process Benchmark Results
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read benchmark results
            const benchmarkData = JSON.parse(fs.readFileSync('output.json', 'utf8'));
            const memoryReport = fs.readFileSync('memory_report.txt', 'utf8');
            const loadTestReport = fs.readFileSync('load_test_report.txt', 'utf8');
            
            // Create markdown report
            const report = `## Performance Benchmark Results
            
            ### AI Model Performance
            ${benchmarkData.benchmarks.map(b => `
            - ${b.name}: ${b.stats.mean.toFixed(4)}s Â±${b.stats.stddev.toFixed(4)}s
            `).join('\n')}
            
            ### Memory Usage
            \`\`\`
            ${memoryReport}
            \`\`\`
            
            ### Load Test Results
            \`\`\`
            ${loadTestReport}
            \`\`\`
            `;
            
            // Create/update comment on PR
            if (context.payload.pull_request) {
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
              });
              
              const benchmarkComment = comments.find(comment => 
                comment.body.includes('Performance Benchmark Results')
              );
              
              if (benchmarkComment) {
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: benchmarkComment.id,
                  body: report,
                });
              } else {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.payload.pull_request.number,
                  body: report,
                });
              }
            }

      - name: Store Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            output.json
            memory_report.txt
            load_test_report.txt
          retention-days: 90

      - name: Performance Regression Check
        if: github.event_name == 'pull_request'
        run: |
          python scripts/check_performance_regression.py \
            --benchmark-results output.json \
            --threshold 10  # 10% regression threshold
